{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd831d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b9dd40",
   "metadata": {},
   "source": [
    "\n",
    "<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: gold ; color : 'black'; text-align: left; border-radius: 2px 2px;pad:4px;\"> Problem Statement </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1bb0c2",
   "metadata": {},
   "source": [
    "<strong>Problem Statement:</strong>\n",
    "\n",
    "We have a lot of customer on our InvestorAI platform. Whenever they login in our app and view anything, we get pings from their mobile phone indicating that they are using the app.\n",
    "<br>\n",
    "You have been provided with 3 weeks of training data and 1 week of test data.\n",
    "<br>\n",
    "<br>\n",
    "<strong>About Data and Variables</strong>\n",
    "\n",
    "Training data contains id, gender, age, number of kids the customer has and all the pings that have been received (during the training data period).\n",
    "<br><br>\n",
    "<strong>Objective:</strong>\n",
    "\n",
    "We want to predict how many hours the customer will be online / using our app on a given day. So the test data contains customer id, and date (during the test data period).\n",
    "The test data also contains the actual online hours, which is what your model should predict.\n",
    "\n",
    "<strong>Evaluation Metric:</strong>\n",
    "\n",
    "We will be looking at Root Mean Squared Error or RMSE for short (lower the better) to see how good your model is.\n",
    "\n",
    "<strong> Special Note:</strong>\n",
    "\n",
    "We have a separate held out test dataset against which we will evaluate your model for overfitting, underfitting, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ae0dd9",
   "metadata": {},
   "source": [
    "<strong>Initial Insights: </strong>\n",
    "\n",
    "1. Looks like we are handling a raw data as custumer database and the activity are seperated out in the data itself. So few assumpitions and data cleaning could be needed ?\n",
    "2. Clearly given in the problem statement that we are going to handle a time related data for each customer.  This could be a multivariante time series problem with multiple timeseries. So modeling stategy and model building need to  be done to capture patterns, ( lets assume)\n",
    "3. Error metric is root mean squred error so its sensitive to small changes, can I optimize my model for better metric, root mean squared log error which is more reponsive to very fine changes?\n",
    "4. As there is another secret hold out set for validation for overfitting and underfitting, so can I hold few datapoints to check this prior?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcfeed3",
   "metadata": {},
   "source": [
    "<p style = \"font-size:25px\"><strong>Lets Dive Into The Data....</strong></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53312a88",
   "metadata": {},
   "source": [
    "\n",
    "<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: gold ; color : 'black'; text-align: left; border-radius: 2px 2px;pad:4px;\"> Data and Initial Stats </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2159a729",
   "metadata": {},
   "source": [
    "loading various module I am planning to use for this assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49171afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing startred...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done,All the required modules are imported. Time elapsed: 12.516072988510132sec\n"
     ]
    }
   ],
   "source": [
    "#importing modules\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "t = time.time()\n",
    "\n",
    "print('Importing startred...')\n",
    "\n",
    "# base libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from scipy import stats\n",
    "from random import randint\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib \n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "\n",
    "\n",
    "# preprocessing libraries\n",
    "from sklearn.model_selection import (TimeSeriesSplit,\n",
    "                                     GridSearchCV,\n",
    "                                     RandomizedSearchCV)\n",
    "\n",
    "from sklearn.preprocessing import (LabelEncoder,\n",
    "                                   StandardScaler, \n",
    "                                   MinMaxScaler, \n",
    "                                   OrdinalEncoder)\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import (mean_squared_error, \n",
    "                             r2_score, \n",
    "                             mean_absolute_error)\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "# modeling algos\n",
    "from sklearn.linear_model import (LogisticRegression,\n",
    "                                  Lasso, \n",
    "                                  ridge_regression,\n",
    "                                  LinearRegression)\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import (AdaBoostRegressor, \n",
    "                              RandomForestRegressor,\n",
    "                              VotingRegressor, \n",
    "                              GradientBoostingRegressor)\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import (LGBMRegressor,\n",
    "                      early_stopping)\n",
    "\n",
    "from sklearn.base import clone ## sklearn base models for stacked ensemble model\n",
    "\n",
    "\n",
    "#Interpretiability of the model\n",
    "import shap\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "\n",
    "## misea\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "\n",
    "print('Done,All the required modules are imported. Time elapsed: {}sec'.format(time.time()-t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b670bc",
   "metadata": {},
   "source": [
    "<p style = \"font-size:25px\"><strong>Data Loading and Visualization....</strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83c57d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "\n",
    "customer_df = pd.read_csv('./data/customers.csv', delimiter = ',', encoding = 'utf-8')\n",
    "pings_df = pd.read_csv('./data/pings.csv', delimiter = ',', encoding = 'utf-8')\n",
    "test_df = pd.read_csv('./data/test.csv', delimiter = ',', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf95f82d",
   "metadata": {},
   "source": [
    "<p style = \"font-size:20px\"><strong>Sneak peak into the data....</strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd28d9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********Data Summary and Inital peaks**********\n",
      "\n",
      "***************Shapes of Data***************\n",
      "\n",
      "Shape of the Customer database: (2500, 4)\n",
      "Shape of the Pings dataset: (50528701, 2)\n",
      "Shape of the Test dataset: (17500, 3)\n",
      "\n",
      "**************************************************\n",
      "\n",
      "Head of Customer database\n",
      "\n",
      "       id gender  age  number_of_kids\n",
      "0  979863   MALE   26               2\n",
      "1  780123   MALE   60               2\n",
      "2  614848   MALE   45               4\n",
      "3  775046   MALE   62               3\n",
      "4  991601   MALE   23               0\n",
      "\n",
      "**************************************************\n",
      "\n",
      "Head of Ping dataset\n",
      "\n",
      "       id   timestamp\n",
      "0  899313  1496278800\n",
      "1  373017  1496278800\n",
      "2  798984  1496278800\n",
      "3  245966  1496278800\n",
      "4  689783  1496278800\n",
      "\n",
      "**************************************************\n",
      "\n",
      "Head of Test dataset\n",
      "\n",
      "       id      date  online_hours\n",
      "0  979863  28/06/17             7\n",
      "1  979863  27/06/17             9\n",
      "2  979863  26/06/17             9\n",
      "3  979863  25/06/17            10\n",
      "4  979863  24/06/17             9\n",
      "\n",
      "**************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('*'*10 + 'Data Summary and Inital peaks' + '*'*10 )\n",
    "\n",
    "print('\\n'+'*'*15 + 'Shapes of Data' + '*'*15+'\\n' )\n",
    "\n",
    "print('Shape of the Customer database: {}'.format(customer_df.shape))\n",
    "print('Shape of the Pings dataset: {}'.format(pings_df.shape))\n",
    "print('Shape of the Test dataset: {}'.format(test_df.shape))\n",
    "\n",
    "print('\\n'+'*'*50 + '\\n')\n",
    "\n",
    "print('Head of Customer database'+ '\\n')\n",
    "print(customer_df.head())\n",
    "\n",
    "print('\\n' +'*'*50+ '\\n')\n",
    "\n",
    "print('Head of Ping dataset'+ '\\n')\n",
    "print(pings_df.head())\n",
    "\n",
    "print('\\n' +'*'*50 + '\\n')\n",
    "\n",
    "print('Head of Test dataset'+ '\\n')\n",
    "print(test_df.head())\n",
    "\n",
    "print('\\n' +'*'*50 + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8686ef8",
   "metadata": {},
   "source": [
    "<strong>Initial Insights about Data</strong>\n",
    "1. As given in problem statement we have three csv files, customer, pings, test datasets. All in common is uniques id, Its customer id.\n",
    "2. Customer database gives all the unique things about customer. This include children, gender, age, of the customer.\n",
    "3. Coming to Ping dataset, data of timestamp is little difficult to compehend as its in unix epoch encodoing, as it is number of seconds from 1970s to specific time, need conversion to make something out of it.\n",
    "4. Test data giving some hints, how I need to convert my training dataset? May be to datatime object.\n",
    "\n",
    "\n",
    "Now questions are how to convert ping data timestamp form epoch timestamp to pandas timestamp? and how to calculate the online hours like test dataset? and finaly need to merge customer dataset to ping dataset and test dataset..\n",
    "\n",
    "\n",
    "first thing first... lets convert ping data timestep to datatime timestamp and extract date, and other basic things later. \n",
    "\n",
    "<strong>Number of Online Hours Calculation Assumption: </strong>\n",
    "\n",
    "Here is the thing, I can calculate online hours directly by taking the one step slide diffrence of timestamps for each date for each customer (by grouping them together). But logically this could be a flawed approch. Reason behind this is no idea when session of custemer end as there is no capping limit given in problem statement. \n",
    "\n",
    "So, assuming if the slide time difference is less than 2minutes (i.e, 120Sec)\n",
    "user active online time is same, if its more than that lets cap it to 2mins.\n",
    "\n",
    "\n",
    "lets implement this idea to the ping dataset... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4e896c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### value sorting with respect to id and timestamp\n",
    "pings_df = pings_df.sort_values(by = ['id','timestamp']).reset_index(drop=True)\n",
    "\n",
    "# creating a copy to preserve actual ping data\n",
    "temp_ping_df = pings_df.copy()\n",
    "\n",
    "#pre-processing data\n",
    "temp_ping_df.drop_duplicates(inplace = True)\n",
    "temp_ping_df['timestamp_decode'] = temp_ping_df['timestamp'].apply(lambda x: datetime.fromtimestamp(x))\n",
    "\n",
    "\n",
    "\n",
    "## timestamp and datagrouping\n",
    "temp_ping_df['date'] = temp_ping_df['timestamp_decode'].dt.date\n",
    "temp_ping_df['online_hours'] = (temp_ping_df.groupby(by=['id','date'])['timestamp'].diff())/(60*60)\n",
    "temp_ping_df['online_hours']  =  temp_ping_df['online_hours'].apply(lambda x: x if x< (2/60) else (2/60))\n",
    "\n",
    "temp_ping_df.fillna(0,inplace = True)\n",
    "\n",
    "\n",
    "#### creating our training data\n",
    "train_df= (temp_ping_df.groupby(by = ['id','date'])['online_hours'].sum()).reset_index()\n",
    "train_df['online_hours'] = round(train_df['online_hours'],1)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0dea66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
